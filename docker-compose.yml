version: "3"
services:


  ofelia:
    image: mcuadros/ofelia:latest
    container_name: ofelia
    depends_on:
      - trino
    command: daemon --docker
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
      
  

  zookeeper:
    image: 'bitnami/zookeeper:latest'
    container_name: zookeeper
    #ports:
    #  - '2181:2181'
    expose:
      - 2181
    environment:
      - ALLOW_ANONYMOUS_LOGIN=yes

  kafka:
    image: 'bitnami/kafka:latest'
    hostname: ${KAFKA_NAME}
    container_name: ${KAFKA_NAME}
    #ports:
    #  - '${KAFKA_PORT}:${KAFKA_PORT}'
    #  - '7071:7071'
    expose:
      - ${KAFKA_PORT}
      - 7071

    environment:
      - KAFKA_BROKER_ID=1
      - KAFKA_CFG_LISTENERS=PLAINTEXT://:${KAFKA_PORT}
      - KAFKA_CFG_ADVERTISED_LISTENERS=PLAINTEXT://:${KAFKA_PORT}
      - KAFKA_CFG_ZOOKEEPER_CONNECT=zookeeper:2181
      - ALLOW_PLAINTEXT_LISTENER=yes
      - KAFKA_OPTS=-javaagent:/opt/bitnami/kafka/libs/kafka-jmx-prometheus-javaagent-1.0.1.jar=7071:/opt/agents/kafka_jmx_exporter.yml
    volumes:
      - ./jmx-exporter/kafka_jmx_exporter.yml/:/opt/agents/kafka_jmx_exporter.yml
      - ./jmx-exporter/jmx_prometheus_javaagent-1.0.1.jar:/opt/bitnami/kafka/libs/kafka-jmx-prometheus-javaagent-1.0.1.jar
      
    depends_on:
      - zookeeper


  init-kafka:
    image: 'bitnami/kafka:latest'
    depends_on:
      - kafka
    entrypoint: [ '/bin/sh', '-c' ]
    command: |
      "
      # blocks until kafka is reachable
      kafka-topics.sh --bootstrap-server ${KAFKA_NAME}:${KAFKA_PORT}  --list

      echo -e 'Creating kafka topics'
      kafka-topics.sh --bootstrap-server ${KAFKA_NAME}:${KAFKA_PORT} --create --if-not-exists --topic ${NOTIFICATION_TOPIC} --replication-factor 1 --partitions 1
      kafka-topics.sh --bootstrap-server ${KAFKA_NAME}:${KAFKA_PORT} --create --if-not-exists --topic ${NOTIFICATION_TOPIC_DEDUP} --replication-factor 1 --partitions 1
      kafka-topics.sh --bootstrap-server ${KAFKA_NAME}:${KAFKA_PORT} --create --if-not-exists --topic ${NOTIFICATION_TOPIC_CONTENT} --replication-factor 1 --partitions 1
      kafka-topics.sh --bootstrap-server ${KAFKA_NAME}:${KAFKA_PORT} --create --if-not-exists --topic ${NOTIFICATION_TOPIC_OUTPUT} --replication-factor 1 --partitions 1
      kafka-topics.sh --bootstrap-server ${KAFKA_NAME}:${KAFKA_PORT} --create --if-not-exists --topic ${NOTIFICATION_TOPIC_TOSTORAGE} --replication-factor 1 --partitions 1
      
      echo -e 'Successfully created the following topics:'
      kafka-topics.sh --bootstrap-server ${KAFKA_NAME}:${KAFKA_PORT} --list
      "

  trino:
    ports:
      - "8080:8080"
    image: "trinodb/trino:449"
    volumes:
#      - ./trino/etc:/usr/lib/trino/etc:ro
      - ./trino/etc:/etc/trino
      - ./trino/catalog:/etc/trino/catalog
      - ./trino/etl.sql:/etl.sql
      - ./trino/etl.sh:/etl.sh
    labels:
      ofelia.enabled: "true"
      ofelia.job-exec.datecron.schedule: "@every 5m"
      ofelia.job-exec.datecron.command: "/bin/bash /etl.sh" 
  

  trino-init:
    image: "trinodb/trino:449"
    depends_on:
      - trino
    volumes:
      - ./trino/init.sql:/init.sql
      - ./trino/init.sh:/init.sh
    entrypoint: [ '/bin/sh', '-c' ]
    command: |
      "
      /bin/bash /init.sh
      "
   

  postgres:
    ports:
      - "5432:5432"
    image: postgres:11
    container_name: postgres
    environment:
      POSTGRES_USER: dbt-trino
      POSTGRES_PASSWORD: dbt-trino
      POSTGRES_DB: dbt-trino

  metastore_db:
    image: postgres:11
    hostname: metastore_db
    environment:
      POSTGRES_USER: hive
      POSTGRES_PASSWORD: hive
      POSTGRES_DB: metastore

  # hive-metastore:
  #     image: apache/hive:${HIVE_VERSION}
  #     depends_on:
  #       - metastore_db
  #     restart: unless-stopped
  #     container_name: hive-metastore
  #     hostname: hive-metastore
  #     expose:
  #       - 8082
  #     environment:
  #       DB_DRIVER: postgres
  #       SERVICE_NAME: 'metastore'
  #       SERVICE_OPTS: '-Xmx1G -Djavax.jdo.option.ConnectionDriverName=org.postgresql.Driver
  #                       -Djavax.jdo.option.ConnectionURL=jdbc:postgresql://metastore_db:5432/metastore
  #                       -Djavax.jdo.option.ConnectionUserName=hive
  #                       -Djavax.jdo.option.ConnectionPassword=hive
  #                       -javaagent:/opt/hive/jmx-prometheus-javaagent-1.0.1.jar=8082:/opt/hive/jmx-exporter-config.yml'        
  #     ports:
  #         - '9083:9083'
  #     volumes:
  #         - ./jmx-exporter/jmx_prometheus_javaagent-1.0.1.jar:/opt/hive/jmx-prometheus-javaagent-1.0.1.jar
  #         - ./jmx-exporter/hivemetastore_jmx_exporter.yml:/opt/hive/jmx-exporter-config.yml
  #         - warehouse:/opt/hive/data/warehouse
  #         - type: bind
  #           source: ${POSTGRES_LOCAL_PATH}
  #           target: /opt/hive/lib/postgres.jar

  hive-metastore:
      hostname: hive-metastore
      image: 'starburstdata/hive:3.1.2-e.18'
      ports:
        - '9083:9083' # Metastore Thrift
      environment:
        HIVE_METASTORE_DRIVER: org.postgresql.Driver
        HIVE_METASTORE_JDBC_URL: jdbc:postgresql://metastore_db:5432/metastore
        HIVE_METASTORE_USER: hive
        HIVE_METASTORE_PASSWORD: hive
        HIVE_METASTORE_WAREHOUSE_DIR: s3://datalake/
        S3_ENDPOINT: http://minio:9000
        S3_ACCESS_KEY: minio
        S3_SECRET_KEY: minio123
        S3_PATH_STYLE_ACCESS: "true"
        REGION: ""
        GOOGLE_CLOUD_KEY_FILE_PATH: ""
        AZURE_ADL_CLIENT_ID: ""
        AZURE_ADL_CREDENTIAL: ""
        AZURE_ADL_REFRESH_URL: ""
        AZURE_ABFS_STORAGE_ACCOUNT: ""
        AZURE_ABFS_ACCESS_KEY: ""
        AZURE_WASB_STORAGE_ACCOUNT: ""
        AZURE_ABFS_OAUTH: ""
        AZURE_ABFS_OAUTH_TOKEN_PROVIDER: ""
        AZURE_ABFS_OAUTH_CLIENT_ID: ""
        AZURE_ABFS_OAUTH_SECRET: ""
        AZURE_ABFS_OAUTH_ENDPOINT: ""
        AZURE_WASB_ACCESS_KEY: ""
        HIVE_METASTORE_USERS_IN_ADMIN_ROLE: "admin"
      depends_on:
        - metastore_db

  minio:
    hostname: minio
    image: 'minio/minio'
    container_name: minio
    ports:
      - '9000:9000'
      - '9001:9001'
    environment:
      MINIO_ROOT_USER: minio
      MINIO_ROOT_PASSWORD: minio123
    command: server /data --console-address ":9001"

  # This job will create the "datalake" bucket on Minio
  mc-job:
    image: 'minio/mc'
    entrypoint: |
      /bin/bash -c "
      sleep 5;
      /usr/bin/mc alias set myminio http://minio:9000 minio minio123 --insecure;
      /usr/bin/mc mb --quiet myminio/datalake --insecure;
      "
    depends_on:
      - minio

  wis2bridge:
    build: wis2bridge/.
    container_name: wis2bridge
    volumes:
      - ./wis2bridge:/app
    expose:
      - ${METRIC_PORT}
    environment:
      - KAFKA_BROKER=${KAFKA_NAME}:${KAFKA_PORT}
      - TOPICS=cache/a/wis2/+/+/data/core/weather/surface-based-observations/synop,cache/a/wis2/+/data/core/weather/surface-based-observations/synop
      #- WIS_BROKER_HOST=globalbroker.meteo.fr
      #- WIS_BROKER_PORT=8883
      #- VALIDATE_SSL=True 
      - WIS_BROKER_HOST=mosquitto
      - WIS_BROKER_PORT=1883
      - VALIDATE_SSL=False
      - WIS_USERNAME=everyone
      - WIS_PASSWORD=everyone
      - LOG_LEVEL=INFO
      - KAFKA_TOPIC=${NOTIFICATION_TOPIC}
      - REPORTING_THRESHOLD=500
      - BATCH_SIZE=50
      - CLIENT_ID=wis2bridge_test_
      - CUSTOMERS_HOST=${KAFKA_NAME}
      - CUSTOMERS_PORT=${KAFKA_PORT}
      - METRIC_PORT=${METRIC_PORT}
    depends_on:
      - kafka
    tty: true

  validation-processor:
    build: 
      context: .
      dockerfile: ./Dockerfile-validation
    container_name: notification-validation
    #volumes:
    #  - ./notification-dedup:/app
    expose:
      - ${METRIC_PORT}
    environment:
      - KAFKA_BROKER=${KAFKA_NAME}:${KAFKA_PORT}
      - KAFKA_TOPIC=${NOTIFICATION_TOPIC}
      - KAFKA_PUBTOPIC=${NOTIFICATION_TOPIC_VALID}
      - KAFKA_ERROR_TOPIC=${ERROR_TOPIC}
      - LOG_LEVEL=INFO
      - POLL_TIMEOUT_SEC=60
      - POLL_BATCH_SIZE=3000
      - CUSTOMERS_HOST=${KAFKA_NAME}
      - CUSTOMERS_PORT=${KAFKA_PORT}
      - METRIC_PORT=${METRIC_PORT}
    depends_on:
      - kafka
    tty: true

  notification-dedup:
    build: 
      context: .
      dockerfile: ./Dockerfile-notification-dedup
    container_name: notification-dedup
    #volumes:
    #  - ./notification-dedup:/app
    expose:
      - ${METRIC_PORT}
    environment:
      - KAFKA_BROKER=${KAFKA_NAME}:${KAFKA_PORT}
      - KAFKA_TOPIC=${NOTIFICATION_TOPIC_VALID}
      - KAFKA_PUBTOPIC=${NOTIFICATION_TOPIC_DEDUP}
      - KAFKA_ERROR_TOPIC=${ERROR_TOPIC}
      - LOG_LEVEL=INFO
      - POLL_TIMEOUT_SEC=60
      - POLL_BATCH_SIZE=3000
      - CUSTOMERS_HOST=${KAFKA_NAME}
      - CUSTOMERS_PORT=${KAFKA_PORT}
      - METRIC_PORT=${METRIC_PORT}
    depends_on:
      - kafka
    tty: true

  content-processor:
    build: 
      context: .
      dockerfile: ./Dockerfile-content-processor
    container_name: content-processor
    #volumes:
    #  - ./content-processor:/app   
    expose:
      - ${METRIC_PORT}
    environment:
      - KAFKA_BROKER=${KAFKA_NAME}:${KAFKA_PORT}
      - KAFKA_TOPIC=${NOTIFICATION_TOPIC_DEDUP}
      - KAFKA_PUBTOPIC=${NOTIFICATION_TOPIC_CONTENT}
      - KAFKA_ERROR_TOPIC=${ERROR_TOPIC}
      - LOG_LEVEL=INFO
      - POLL_TIMEOUT_SEC=10
      - POLL_BATCH_SIZE=50
      - NR_THREADS=2
      - CUSTOMERS_HOST=${KAFKA_NAME}
      - CUSTOMERS_PORT=${KAFKA_PORT}
      - METRIC_PORT=${METRIC_PORT}
    depends_on:
      - kafka
    tty: true

  decoding-processor:
    build: 
      context: .
      dockerfile: ./Dockerfile-decoding-processor
    container_name: decoding-processor
    #volumes:
    #  - ./decoding-processor:/app
    expose:
      - ${METRIC_PORT}
    environment:
      - KAFKA_BROKER=${KAFKA_NAME}:${KAFKA_PORT}
      - KAFKA_TOPIC=${NOTIFICATION_TOPIC_CONTENT}
      - KAFKA_PUBTOPIC=${NOTIFICATION_TOPIC_OUTPUT}
      - LOG_LEVEL=INFO
      - POLL_TIMEOUT_SEC=10
      - POLL_BATCH_SIZE=50
      - CUSTOMERS_HOST=${KAFKA_NAME}
      - CUSTOMERS_PORT=${KAFKA_PORT}
      - METRIC_PORT=${METRIC_PORT}
    depends_on:
      - kafka
    tty: true


  output-processor:
    build: 
      context: .
      dockerfile: ./Dockerfile-output-processor
    container_name: output-processor
    #volumes:
    #  - ./output-processor:/app
    expose:
      - ${METRIC_PORT}
    environment:
      - KAFKA_BROKER=${KAFKA_NAME}:${KAFKA_PORT}
      - KAFKA_TOPIC=${NOTIFICATION_TOPIC_OUTPUT}
      - KAFKA_PUBTOPIC=${NOTIFICATION_TOPIC_TOSTORAGE}
      - LOG_LEVEL=INFO
      - POLL_TIMEOUT_SEC=10
      - POLL_BATCH_SIZE=50
      - CUSTOMERS_HOST=${KAFKA_NAME}
      - CUSTOMERS_PORT=${KAFKA_PORT}
      - METRIC_PORT=${METRIC_PORT}
    depends_on:
      - kafka
    tty: true

  mosquitto:
    image: eclipse-mosquitto
    hostname: mosquitto
    container_name: mosquitto
    restart: unless-stopped
    ports:
      - "1883:1883"
      #- "9001:9009"
    volumes:
      - ./test-broker:/etc/mosquitto
      - ./test-broker/mosquitto.conf:/mosquitto/config/mosquitto.conf
    

  mqtt-producer:
    build: ./test-node/.

    volumes:
      - ./test-node:/app

    container_name: mqtt-producer
    working_dir: /app
    command: bash publoop.sh

    environment:
      #- SLEEPRATE=0.1
      - CONTENT_INTEGRATED_RATE=0.2 # % of messages should have the content included
      - INTEGRITY_CHECKSUM_RATE=0.001 # % of messates should have wrong checksum
      - INTEGRITY_LENGTH_CONTENT_RATE=0.001 # % of messages should have wrong lenght in content
      - INTEGRITY_LENGTH_LINK_RATE=0.001 # % of messages should have wrong lenght in content
      - INTEGRITY_SCHEMA_RATE=0.001 # % of messages with incorrect schema
      - INTEGRITY_PUBDATE_RATE=0.001 # % of messages with non ISO pubdate
      - NR_DUPLICATES_MAX=3
      - NR_DUPLICATES_MIN=1
      - DUPLICATE_MAX_DELAY_MS=1000
      - NR_CACHES=3
      - MSG_RATE=10 # messages per second

    depends_on:
      - mosquitto

    tty: true


  test-cache:
    hostname: test-cache
    container_name: test-cache
    image: nginx
    expose:
      - "8080:80"
    #ports:
    #  - "8080:80"
    volumes:
      - ./test-cache/njs/:/etc/nginx/njs/:ro
      - ./test-cache/cache-files:/usr/share/nginx/html
      - ./test-cache/nginx.conf:/etc/nginx/nginx.conf
    environment:
      - NGINX_HOST=test-cache
      - FAIL_RATE=0.5

    networks:
      default:
        aliases:
          - test-cache-1
          - test-cache-2
          - test-cache-3

  prometheus:
    image: prom/prometheus
    container_name: prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
    ports:
      - 9090:9090
    expose:
      - 9090
    restart: unless-stopped
    volumes:
      - ./prometheus:/etc/prometheus
    #  - prom_data:/prometheus
  grafana:
    image: grafana/grafana
    container_name: grafana
    ports:
      - 3000:3000
    expose:
      - 3000
    restart: unless-stopped
    environment:
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=grafana
    volumes:
      - ./grafana:/etc/grafana/provisioning/datasources
    #  - 'grafana_storage:/var/lib/grafana'

volumes:
  grafana_storage: {}
  hive-db:
  warehouse:

#  prom_data:

networks:
  default:
    
